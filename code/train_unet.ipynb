{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa07563",
   "metadata": {},
   "source": [
    "# U-Net Training for Ultrasound Image Reconstruction\n",
    "This notebook sets up the baseline training pipeline for U-Net using ultrasound data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012cc046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# for dataset preprocessing\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "\n",
    "# evaluation metrics for reconstructed images\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20165ee3",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Define all the training parameters in one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'model_name': 'unet',\n",
    "    'batch_size': 4,\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 150,\n",
    "    'input_size': (128, 128),\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'data_dir_in':  os.path.join('..', 'data', 'raw'),\n",
    "    'data_dir_out': os.path.join('..', 'data', 'processed'),\n",
    "    'checkpoint_dir': './checkpoints/unet/',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bd820",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "THIS NEEDS REPLACING! We assume .mat or .npy for input and .npy or .png for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltrasoundDataset(Dataset):\n",
    "    def __init__(self, rf_dir, img_dir, transform=None):\n",
    "        self.rf_paths = sorted([\n",
    "            os.path.join(rf_dir, f) for f in os.listdir(rf_dir) if f.endswith('.mat')\n",
    "        ])\n",
    "        self.img_paths = sorted([\n",
    "            os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith('.png') or f.endswith('.npy')\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "        assert len(self.rf_paths) == len(self.img_paths), \"Mismatched dataset sizes!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rf_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load RF .mat file\n",
    "        rf = sio.loadmat(self.rf_paths[idx])\n",
    "        rf = rf[list(rf.keys())[-1]]  # or use exact key if known (e.g., rf['rf'])\n",
    "\n",
    "        # Load GT image\n",
    "        if self.img_paths[idx].endswith('.png'):\n",
    "            img = np.array(Image.open(self.img_paths[idx])) / 255.0\n",
    "        else:\n",
    "            img = np.load(self.img_paths[idx])\n",
    "\n",
    "        # Ensure shape: (C, H, W)\n",
    "        rf = torch.tensor(rf, dtype=torch.float32).unsqueeze(0) if rf.ndim == 2 else torch.tensor(rf, dtype=torch.float32)\n",
    "        img = torch.tensor(img, dtype=torch.float32).unsqueeze(0) if img.ndim == 2 else torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "        return rf, img\n",
    "\n",
    "train_loader = DataLoader(UltrasoundDataset('train'), batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(UltrasoundDataset('val'), batch_size=CONFIG['batch_size']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00123e33",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "Here we define the model architecture used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aad477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for U-Net (same cell can be replaced with ResNet later)\n",
    "from model_library import UNet  # optional if in separate file\n",
    "\n",
    "model = UNet(in_channels=1, out_channels=1).to(CONFIG['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # or SSIM, L1Loss, etc.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for x, y in tqdm(loader):\n",
    "        x, y = x.to(CONFIG['device']), y.to(CONFIG['device'])\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total_psnr, total_ssim = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(CONFIG['device']), y.to(CONFIG['device'])\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # PSNR & SSIM (on CPU, single example per batch assumed)\n",
    "            x_np = y_pred.cpu().numpy()\n",
    "            y_np = y.cpu().numpy()\n",
    "            for i in range(x_np.shape[0]):\n",
    "                p, s = compute_metrics(y_np[i], x_np[i])\n",
    "                total_psnr += p\n",
    "                total_ssim += s\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return (\n",
    "        val_loss / len(loader),\n",
    "        total_psnr / n,\n",
    "        total_ssim / n\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Expects numpy arrays with shape (1, H, W)\n",
    "    \"\"\"\n",
    "    y_true = y_true.squeeze()\n",
    "    y_pred = y_pred.squeeze()\n",
    "\n",
    "    _psnr = psnr(y_true, y_pred, data_range=1.0)\n",
    "    _ssim = ssim(y_true, y_pred, data_range=1.0)\n",
    "    return _psnr, _ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3e639",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "In this part we set up the training loop, train the model and then save the parameters of the final one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(CONFIG['epochs']):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_psnr, val_ssim = validate(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | PSNR: {val_psnr:.2f} | SSIM: {val_ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc9a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = os.path.join(CONFIG['checkpoint_dir'], 'final_model.pt')\n",
    "torch.save(model.state_dict(), final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350cf62",
   "metadata": {},
   "source": [
    "## Model Visualization\n",
    "We visualize one example of the reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c93d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(model, loader):\n",
    "    model.eval()\n",
    "    x, y = next(iter(loader))\n",
    "    x = x.to(CONFIG['device'])\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x).cpu()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axs[0].imshow(x[0, 0].cpu(), cmap='gray')\n",
    "    axs[0].set_title(\"Input\")\n",
    "    axs[1].imshow(y[0, 0], cmap='gray')\n",
    "    axs[1].set_title(\"Target\")\n",
    "    axs[2].imshow(y_pred[0, 0], cmap='gray')\n",
    "    axs[2].set_title(\"Prediction\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
